# -*- coding: utf-8 -*-
"""word2vecSentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qUEN53WU_XiJCSmkmFCeryoQr8Z9YHNd
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My Drive/NLP/SA_Word2Vec/data1

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# gensim modules
from gensim import utils
from gensim.models.doc2vec import LabeledSentence
from gensim.models import Doc2Vec

# numpy
import numpy as np

# classifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# random, itertools, matplotlib
import random
import itertools
import matplotlib.pyplot as plt

import codecs
import re
import os    
from chardet import detect

# get file encoding type
def get_encoding_type(file):
    with open(file, 'rb') as f:
        rawdata = f.read()
    return detect(rawdata)['encoding']

if __name__ == '__main__':
  cnt = 0
  file= open('rt-polarity.pos', 'r', encoding = get_encoding_type('rt-polarity.pos'), errors='ignore')
  for line in file:
    line =  re.sub(r'[?|$|.|!]',r'',line + '\n')
    line = re.sub(r'[^a-zA-Z0-9 ]',r'',line)
    cnt += 1
  file.close()
  print(cnt)

class LabeledLineSentence(object):
    def __init__(self, sources):
        self.sources = sources
        
        flipped = {}
        
        # make sure that keys are unique
        for key, value in sources.items():
            if value not in flipped:
                flipped[value] = [key]
            else:
                raise Exception('Non-unique prefix encountered')
    
    def __iter__(self):
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])
    
    def to_array(self):
        self.sentences = []
        for source, prefix in self.sources.items():
            with utils.smart_open(source) as fin:
                for item_no, line in enumerate(fin):
                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))
        return self.sentences
    
    def sentences_perm(self):
        shuffled = list(self.sentences)
        random.shuffle(shuffled)
        return shuffled

sources = {
    'test-neg.txt':'TEST_NEG',
    'test-pos.txt':'TEST_POS', 
    'train-neg.txt':'TRAIN_NEG', 
    'train-pos.txt':'TRAIN_POS', 
}

sentences = LabeledLineSentence(sources)

sources = {
    'neg.txt':'TRAIN_NEG',
    'pos.txt':'TRAIN_POS'
}
sentences = LabeledLineSentence(sources)

model = Doc2Vec(min_count=5, window=8, size=100, sample=1e-4, negative=5, workers=4)
model.build_vocab(sentences.to_array())

model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs=20)

model.most_similar('good')
model.docvecs['TRAIN_NEG_0']

model.save('./imdb.d2v')
model = Doc2Vec.load('./imdb.d2v')

import numpy as np
from sklearn.model_selection import train_test_split

X = np.zeros((10660, 100))
y = np.zeros(10660)
for i in range(5330):
   X[i] = model.docvecs['TRAIN_POS_' + str(i)]
   y[i] = 1
   X[5330+i] = model.docvecs['TRAIN_NEG_' + str(i)]
   y[i+5330] = 0

X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.33, random_state=100)

X_train = np.zeros((25000, 100))
y_train = np.zeros(25000)
for i in range(12500):
    prefix_train_pos = 'TRAIN_POS_' + str(i)
    prefix_train_neg = 'TRAIN_NEG_' + str(i)
    X_train[i] = model.docvecs[prefix_train_pos]
    X_train[12500 + i] = model.docvecs[prefix_train_neg]
    y_train[i] = 1
    y_train[12500 + i] = 0

X_test = np.zeros((25000, 100))
y_test = np.zeros(25000)

for i in range(12500):
    prefix_test_pos = 'TEST_POS_' + str(i)
    prefix_test_neg = 'TEST_NEG_' + str(i)
    X_test[i] = model.docvecs[prefix_test_pos]
    X_test[12500 + i] = model.docvecs[prefix_test_neg]
    y_test[i] = 1
    y_test[12500 + i] = 0

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

print ('Accuracy', classifier.score(X_test, y_test))
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
plot_confusion_matrix(cm, classes=['neg', 'pos'])

classifier = SVC()
classifier.fit(X_train, y_train)

print ('Accuracy', classifier.score(X_test, y_test))
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
plot_confusion_matrix(cm, classes=['neg', 'pos'])